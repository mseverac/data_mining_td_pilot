{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Guided exercice\n",
    " This exercice comes from « Practical Data Mining with Python » , Giuseppe Vettigli \n",
    "   \n",
    " ## Data imports \n",
    "The file describes 50 specimen of iris, belonging to 3 different species (Iris setosa, Iris virginica and Iris versicolor). Each specimen is described with 4 caracteristics: \n",
    "    - length of Sepal,\n",
    "    - width of Sepal,\n",
    "    - length of Petals,\n",
    "    - width of Petal.\n",
    " \n",
    " The specie is the 5th data.\n",
    "\n",
    " Data is presented in a csv file. To facilitate the use, it will be store in a panda dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of the needed libraires\n",
    "#graphical librairies\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import figure, subplot, hist, xlim, show, plot\n",
    "%matplotlib inline\n",
    "\n",
    "#data librairies \n",
    "\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import boxplot\n",
    "from pandas.plotting import parallel_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data importand creation of panda object\n",
    "data_panda = pd.read_csv('iris_english.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that import went well. \n",
    "Display the columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data_panda.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_panda.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the names of species and their repartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_panda['Species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_panda.groupby('Species').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the colors used for visualization \n",
    "colors = np.where(data_panda['Species']=='setosa','r','-')\n",
    "colors[data_panda['Species']=='versicolor'] = 'g'\n",
    "colors[data_panda['Species']=='virginica']= 'b'\n",
    "#print(colors)\n",
    "color_dict={'setosa':'r','versicolor':'g' ,'virginica':'b'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Visualization enables to better understand the input data  and to verify the need of pre-treatments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to facilitate futur use, we can create a set with the input columns\n",
    "Input_cols = ['Sepal_lenght', 'Sepal_width', 'Petal_lenght', 'Petal_width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of input data is very important. 2 things should be considered :\n",
    "the target value should be excluded from this set.\n",
    "random data or data unrelated to the target should not be included.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_panda.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on représente ainsi la longeur des pétales  en fonction de la longueur des sépales \n",
    "\n",
    "data_panda.plot(kind=\"scatter\", x='Sepal_lenght', y='Petal_lenght')\n",
    "\n",
    "plt.title('150 specimens') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the defined colors to stress out the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Sepal_lenght', y='Petal_lenght', hue='Species', palette=color_dict, data=data_panda) \n",
    "\n",
    "plt.title('150 specimens') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use histograms and compare the distribution .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(data_panda, figsize=(10, 10), diagonal='hist', c=colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_panda.boxplot(by='Species', figsize=(12, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphics can give us ideas to ease the classification. for instance we can see that  Iris Setosa have smaller Sepal than Iris Virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre treatment\n",
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To garanty that the use of Euclidan distances will not favor one characteristics, we need to work on normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalisation\n",
    "import copy\n",
    "Norm=copy.deepcopy(data_panda)\n",
    "Norm[Input_cols]=(data_panda[Input_cols]-data_panda[Input_cols].min())/(data_panda[Input_cols].max()-data_panda[Input_cols].min())\n",
    "print(Norm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Sepal_lenght', y='Petal_lenght', hue='Species', palette=color_dict, data=Norm) \n",
    "plt.title('150 specimens, normalized') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Norm.boxplot(by='Species', figsize=(12, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "Some methods can only use numerical data. In order to use them we need to transform the target values into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #transformation des espèces en numéro de classe\n",
    "Norm.Species=Norm.Species.astype('category')\n",
    "Norm['Species_encoded'],dict_cat=Norm.Species.factorize()\n",
    "  # le dictionnaire d encodage est stocké dans le vecteur dict_cat\n",
    "print(dict_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #créons le dictionnaire des couleurs associé\n",
    "color_dict_encoded={}\n",
    "for i in range (0, 3):\n",
    "      color_dict_encoded[i]=color_dict[dict_cat[i]]\n",
    "print(color_dict_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis project data in a space where variance is maximized. \n",
    "This means that if 2 points are different in a n-dimension space they should not overlap in the 2-dimension PCA space.\n",
    "\n",
    "Lets determine the lost information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "for i in range(1,5):\n",
    "\n",
    "    pca = PCA(n_components=i)\n",
    "\n",
    "    pca.fit(Norm[Input_cols])\n",
    "\n",
    "    print (i, 'components representa data loss of' ,(1-sum(pca.explained_variance_ratio_)) * 100,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 2-D representation for visualization.\n",
    "If the algortihmes take too long we should decide which PCA recduction is an acceptable loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=2\n",
    "pca = PCA(n_components)\n",
    "pca.fit(Norm[Input_cols])\n",
    "pca_apply = pca.transform(Norm[Input_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify the composition of those 2 components from the 4 initial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base=pd.DataFrame(pca.components_,columns=Norm[Input_cols].columns,index = ['PCA0','PCA1'])            \n",
    "print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcad_panda=pd.DataFrame(pca_apply, columns=['PCA%i' % i for i in range(n_components)]) #save in a panda object\n",
    "Norm=pd.concat([Norm, pcad_panda], axis=1)#concatenate in norm_pd\n",
    "print(Norm.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "sns.scatterplot(x='PCA0',y='PCA1', hue='Species',palette=color_dict, data=Norm)\n",
    "pl.xlabel('PCA0')\n",
    "pl.ylabel('PCA1')\n",
    "pl.title(' 150 specimens in the new base')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Classification\n",
    "\n",
    "Classification enables to assoign a category to a specimen. 2 steps are required:\n",
    "- Learning\n",
    "- Prediction.\n",
    "\n",
    "Python librairy \"sklearn\" has numerous classification models. Here we will use Gaussian Naive Bayes to determine the species of the iris.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The data should then be split in 2 groups:\n",
    "- learning set\n",
    "- test set\n",
    "\n",
    "This split can be done manually by an expert (which will be the case for the car example) or can be done randomly wiht the \"train_test_split\" function.\n",
    "\n",
    "In this case, the expert should only choose the  % of each group. Here we choose  40% of the specimens for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Learning population is called train,\n",
    "#the target value (species) t_train\n",
    "#test population is called test\n",
    "#the predicted value t_test\n",
    "\n",
    "train, test, t_train, t_test = train_test_split(Norm, Norm['Species_encoded'], test_size=0.4, random_state=0)\n",
    "#print(train)\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lest visualize the repartition between the 2 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='PCA0',y='PCA1', data=train)\n",
    "sns.scatterplot(x='PCA0',y='PCA1', data=test)\n",
    "\n",
    "pl.xlabel('PCA0')\n",
    "pl.ylabel('PCA1')\n",
    "plt.legend( loc='upper left', labels=['Learning set', 'Test set'])\n",
    "pl.title('Random repartition of specimens') \n",
    "\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st step - learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_GNB=GaussianNB()\n",
    "#Learning\n",
    "classifier_GNB.fit(train[Input_cols],train['Species_encoded']) # train\n",
    "#Prediction\n",
    "prediction_train =classifier_GNB.predict(train[Input_cols]) #prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we can compare the preduction and real specy for the first specimen\n",
    "print('The first flower is a '+train['Species'][0]+' (encoded as '+ str(train['Species_encoded'][0])+')')\n",
    "print('The Naive Bayes method predicts it as '+ dict_cat[prediction_train [0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure the performance  on all the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classifier_GNB.score(train[Input_cols],t_train)) # test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is the number of correct preductions divied by the number of specimens in the test set.\n",
    "This is the proportion of accurate prediction.\n",
    "If we are happy with the result we can apply it on the test set to ensure that there are no aver-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test =classifier_GNB.predict(test[Input_cols]) #prediction\n",
    "print (classifier_GNB.score(test[Input_cols],t_test)) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction =classifier_GNB.predict(Norm[Input_cols]) #prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An evaluation tool is the confusion matrix. In this matrix columns are predicted classes and rows the real classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "M_GNB=confusion_matrix(prediction_test,t_test)\n",
    "print (M_GNB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also represent this info in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_GNB = pd.DataFrame(columns=['real','real_name','predicted','density'])\n",
    "for i in range (0, 3):\n",
    "    for j in range (0,3) :\n",
    "        if M_GNB[i][j]>0 :\n",
    "            new_row =pd.DataFrame({'real':i, 'real_name':dict_cat[i],'predicted':j, 'density':float(M_GNB[i][j])}, index=[0])\n",
    "            conf_GNB=pd.concat([conf_GNB,new_row], ignore_index=True)\n",
    "sns.scatterplot(x='real_name', y='predicted', s=(conf_GNB.density)*60, data=conf_GNB) \n",
    "pl.xlabel('Real species')\n",
    "pl.ylabel('Predicted species')\n",
    "pl.title('Prediction relevance')\n",
    "show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this representation, the errors are the non diagonal elements. Here iris versicolor were labbeled as virginica. \n",
    "\n",
    "Other metrics can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report(prediction_test,t_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision : proportion of the class attributed rightfully\n",
    "- Recall : proportion of elements of this class wrongfully  attributed\n",
    "- F1-Score: Harmonic mean of the 2 other indicators\n",
    "- support : numbre of element of this class used in the test.\n",
    "\n",
    "To be relevant the evaluation should be done in multiple pairs (learning set/test set). \n",
    "We can then use \"Cross Validation\". It split the initial population several times. The performance of the classfication is the mean of the several evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross validation with 6 iterations \n",
    "scores = cross_val_score(classifier_GNB,Norm[Input_cols], Norm['Species_encoded'], cv=6)\n",
    "\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a vector with the perf for each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "\n",
    "print (mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use\n",
    "If we are happy with the classification we can use it to classify the whole set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_GNB =classifier_GNB.predict(Norm[Input_cols]) #prediction\n",
    "pred_GNB = pd.DataFrame(prediction_test_GNB )\n",
    "pred_GNB.columns = ['Prediction_GNB']\n",
    "\n",
    "#we merge this dataframe with df\n",
    "Norm= pd.concat([Norm,pred_GNB], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use it for new specimen. For instance, what is the species of a flower with  Petal and Sepal normalized lenght  and width of 0.5.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_specimen = {'Sepal_lenght': [0.5],\n",
    "        'Sepal_width': [0.5],\n",
    "        'Petal_lenght': [0.5],\n",
    "        'Petal_width': [0.5],\n",
    "        }\n",
    "panda_New_specimen = pd.DataFrame(New_specimen)  \n",
    "D=classifier_GNB.predict(panda_New_specimen)\n",
    "print('Using Gaussian Naive Bayes, the predicted species of such a flower is '+ dict_cat[D[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron network\n",
    "For Neuron network we follow the same method: learn on a train set, apply on a test set and if we are happy with the performance use it for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier_NN = MLPClassifier()\n",
    "#learn\n",
    "classifier_NN.fit(train[Input_cols],train['Species_encoded']) # learning classifier.fit(input_dat, target_data)\n",
    "#use on test\n",
    "prediction =classifier_NN.predict(test[Input_cols]) #prediction\n",
    "#evaluate on test\n",
    "print (classifier_NN.score(test[Input_cols],t_test)) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#application on the whole set\n",
    "prediction_NN =classifier_NN.predict(Norm[Input_cols]) #prediction\n",
    "pred_NN = pd.DataFrame(prediction_NN )\n",
    "pred_NN.columns = ['Prediction_NN']\n",
    "\n",
    "#we merge this dataframe with df\n",
    "Norm= pd.concat([Norm,pred_NN], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_NN=confusion_matrix(Norm['Species_encoded'],Norm['Prediction_NN'])\n",
    "print (M_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confNN = pd.DataFrame(columns=['real','real_species','predicted','predictionNN','density'])\n",
    "for i in range (0, 3):\n",
    "    for j in range (0,3) :\n",
    "        if M_NN[i][j]>0 :\n",
    "            new_row = pd.DataFrame({'real':i, 'real_species':dict_cat[i],'predicted':j,'predicted_cluster':dict_cat[j], 'density':float(M_NN[i][j])}, index=[0])\n",
    "            confNN=pd.concat([confNN,new_row], ignore_index = True)             \n",
    "sns.scatterplot(x='real_species', y='predicted_cluster', s=(confNN.density)*60, data=confNN) \n",
    "pl.xlabel('Real species')\n",
    "pl.ylabel('Cluster k-mean')\n",
    "pl.title('Alignement between clusters and species')\n",
    "show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "When initial data are not labelled, groups need to be created base on similarity.\n",
    "This is unsupervised learning.\n",
    "Here we will use a classical clustering analysis method: k-mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Norm[Input_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Norm.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import completeness_score, homogeneity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nombre_clusters=3#cluster nombers matching rhe numbers of species\n",
    "kmeans = KMeans(n_clusters=Nombre_clusters, init='random',n_init='auto') # initialization \n",
    "kmeans.fit(Norm[Input_cols]) #K-means training\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    \n",
    "print('Coordinates of the  3 centroids')\n",
    "print(centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual prediction\n",
    "y_pred = kmeans.predict(Norm[Input_cols])\n",
    "#We store the K-means results in a dataframe\n",
    "pred = pd.DataFrame(y_pred)\n",
    "pred.columns = ['Prediction_kmean']\n",
    " \n",
    "\n",
    "#we merge this dataframe with df\n",
    "Norm = pd.concat([Norm,pred], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.scatterplot(x='PCA0', y='PCA1', hue='Prediction_kmean', data=Norm) \n",
    "pl.title('3 Clusters K-Means')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare these clusterswith the reality to obtain the performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (completeness_score(Norm['Species_encoded'],Norm['Prediction_kmean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completeness is near to 1 when all elements of a class belong to the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (homogeneity_score(Norm['Species_encoded'],Norm['Prediction_kmean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogeneity is near to 1 when all elements of a cluster belong to the same class.\n",
    "\n",
    "Visualization also enables to confront clustering to reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize = (10, 10))\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "sns.scatterplot(x='PCA0', y='PCA1', hue='Species_encoded', palette=color_dict_encoded, data=Norm) \n",
    "plt.title('Real species')\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "sns.scatterplot(x='PCA0', y='PCA1', hue='Prediction_kmean', data=Norm) \n",
    "plt.title('3 Clusters K-Means')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2=confusion_matrix(Norm['Species_encoded'],Norm['Prediction_kmean'])\n",
    "print (M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!BEWARE!!!!\n",
    "Here perfect identification does not mean diagonal matrix. It does not matter if setosa are clustern°1, n° 2 or n°3. \n",
    "\n",
    "Let's represent it graphicaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cluster={0: 'A', 1: 'B', 2: 'C'}\n",
    "conf1 = pd.DataFrame(columns=['real','real_species','predicted','predicted_cluster','density'])\n",
    "for i in range (0, 3):\n",
    "    for j in range (0,3) :\n",
    "        if M2[i][j]>0 :\n",
    "            new_row = pd.DataFrame({'real':i, 'real_species':dict_cat[i],'predicted':j,'predicted_cluster':dict_cluster[j], 'density':float(M2[i][j])}, index=[0])\n",
    "            conf1=pd.concat([conf1,new_row], ignore_index = True)             \n",
    "sns.scatterplot(x='real_species', y='predicted_cluster', s=(conf1.density)*60, data=conf1) \n",
    "pl.xlabel('Real species')\n",
    "pl.ylabel('Cluster k-mean')\n",
    "pl.title('Alignement between clusters and species')\n",
    "show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(Norm['Species_encoded'],Norm['Prediction_kmean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To \"match\" the clusters to the real labels (here species) we can use the following function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function find the best fit between clusters and labels\n",
    "from itertools import permutations # import this into script.\n",
    "#tested with python 3.6\n",
    "def remap_labels(pred_labels, true_labels):\n",
    "    \"\"\"Rename prediction labels (clustered output) to best match true labels.\"\"\"\n",
    "   \n",
    "    pred_labels, true_labels = np.array(pred_labels), np.array(true_labels)\n",
    "    assert pred_labels.ndim == 1 == true_labels.ndim\n",
    "    assert len(pred_labels) == len(true_labels)\n",
    "    cluster_names = np.unique(pred_labels)\n",
    "    accuracy = 0\n",
    "\n",
    "    perms = np.array(list(permutations(np.unique(true_labels))))\n",
    "\n",
    "    remapped_labels = true_labels\n",
    "    for perm in perms:\n",
    "        flipped_labels = np.zeros(len(true_labels))\n",
    "        for label_index, label in enumerate(cluster_names):\n",
    "            flipped_labels[pred_labels == label] = perm[label_index]\n",
    "\n",
    "        testAcc = np.sum(flipped_labels == true_labels) / len(true_labels)\n",
    "        if testAcc > accuracy:\n",
    "            accuracy = testAcc\n",
    "            remapped_labels = flipped_labels            \n",
    "            dict_map= dict(enumerate(perm, 0))\n",
    "            #print(dict_map)\n",
    "\n",
    "    return accuracy, remapped_labels,dict_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc,y_pred,dict_map_cluster =remap_labels(Norm['Prediction_kmean'],Norm['Species_encoded'])\n",
    "print(dict_map_cluster)\n",
    "#We store the K-means results in a dataframe\n",
    "pred = pd.DataFrame(y_pred)\n",
    "pred.columns = ['Prediction_kmean_mapped']\n",
    "\n",
    "#we merge this dataframe with df\n",
    "Norm= pd.concat([Norm,pred], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionnary gives us the relationship between clusters and species. We can use it to represent the \"traditional\" confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_KM=confusion_matrix(Norm['Species_encoded'],Norm['Prediction_kmean_mapped'])\n",
    "print (M_KM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_KM = pd.DataFrame(columns=['real','real_name','predicted','density'])\n",
    "for i in range (0, 3):\n",
    "    for j in range (0,3) :\n",
    "        if M_KM[i][j]>0 :\n",
    "            new_row = pd.DataFrame({'real':i, 'real_name':dict_cat[i],'predicted':j, 'density':float(M_KM[i][j])}, index=[0])\n",
    "            conf_KM=pd.concat([conf_KM,new_row], ignore_index=True)\n",
    "            \n",
    "print(conf_KM)\n",
    "sns.scatterplot(x='real_name', y='predicted', s=(conf_KM.density)*60, data=conf_KM) \n",
    "pl.xlabel('Real species')\n",
    "pl.ylabel('Cluster k-mean')\n",
    "pl.title('Prediction accuracy')\n",
    "show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clustering also enables to predict label of a new specimen.the Cette classification de la population d'apprentissage permet également de prédire l appartenance d 'un nouveau spécimen. \n",
    "For instance, what is the cluster of a flower with  Petal and Sepal normalized lenght  and width of 0.5.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "D_kmeans=kmeans.predict(panda_New_specimen)\n",
    "print('Using kmeans, the predicted species of such a flower is '+ dict_cat[dict_map_cluster[D_kmeans[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines create a .csv file that encompass the predicted species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_specimen=len(data_panda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# writing the csv file\n",
    "with open('my_prediction.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # writing the header\n",
    "    writer.writerow(['Flower', 'Prediction GNB', 'Prediction k-means','Prediction NN'])\n",
    "    # writing the data\n",
    "    for i in range(nb_specimen):\n",
    "        writer.writerow([i, dict_cat[Norm.loc[i,'Prediction_GNB']], dict_cat[int(Norm.loc[i,'Prediction_kmean_mapped'])],dict_cat[int(Norm.loc[i,'Prediction_NN'])]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
